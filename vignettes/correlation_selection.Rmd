---
title: "Selecting the Best Correlation"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Selecting the Best Correlation}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---


If after seeing the heatmaps that show the best and worst areas of applicability of any of the correlation does not convince you, I am showing a diiferent way of visualizing the correlations.

This time I am using all the statistical variables from the table generated by z.stats for the Hall-Yarborough correlation:

```{r}
library(zFactor)
library(tibble)

as.tibble(z.stats("HY"))
```

We could use another function of zFactor to calculate the statistics of the statical variable for all correlations, in this case the *Mean Absolute Percentage Error*.

```{r}
stats_of_z.stats("MAPE")
```

The inconvenience here is that these statiscal function may not be able easy to interpret. We could try instead applying `quantile` to all the statistical variables to all the correlation and then show a boxplot. 

Now, what if we take t

We use this time the function `z.stats_quantile`


## Boxplots for MPE at different `y` scale

```{r}
library(zFactor)
qcorrs <- z.stats_quantile("MPE")

# par(mfrow = c(1,3))
boxplot(qcorrs,  ylim= c(-600, 100), cex = 1.5, las=2, main = "MPE, y = (-4, 100)")
grid()
boxplot(qcorrs,  ylim= c(-4, 30), cex = 1.5, las=2, main = "MPE, y = (-4, 30)")
grid()
boxplot(qcorrs,  ylim= c(-4, 6), cex = 1.5, las=2, main = "MPE, y = (-4, 6)")
# grid()
```

## Boxplots for four statistical indicators for all correlations

```{r}
library(zFactor)

qcorrs <- z.stats_quantile("RMSE")
# op <- par(mfrow = c(1,4))
boxplot(qcorrs, log = "y", ylim =  c(1e-6, 1e3), las=2, main = "RMSE")
grid()

qcorrs <- z.stats_quantile("MAPE")
boxplot(qcorrs, log = "y", ylim =  c(1e-6, 1e3), las=2, main = "MAPE")
grid()

qcorrs <- z.stats_quantile("MSE")
boxplot(qcorrs, log = "y", ylim = c(1e-12, 1e3), las=2, main = "MSE")
grid()

qcorrs <- z.stats_quantile("RSS")
boxplot(qcorrs, log = "y", ylim = c(1e-12, 1e3), las=2, main = "RSS")
grid()
# par(op)
```

What we find out here is that four of the correlations show plenty of variation. The trends are similars since the statistical function originate from the sequare of the error.

But this still could not help us for the selection.
Let's try another angle. But focusing on one statistical measurement: Mean Percentage Error.

## Boxplots for MPE at different `y` scale
The MPE is the only statistic indicator that will show negative values. So, we could appreciate the dispersion. Let's plot the MPE for all the correlations.

```{r}
library(zFactor)
qcorrs <- z.stats_quantile("MPE")

# par(mfrow = c(1,3))
boxplot(qcorrs,  ylim= c(-600, 100), cex = 1.5, las=2, main = "MPE, y = (-4, 100)")
grid()
boxplot(qcorrs,  ylim= c(-4, 30), cex = 1.5, las=2, main = "MPE, y = (-4, 30)")
grid()
boxplot(qcorrs,  ylim= c(-4, 6), cex = 1.5, las=2, main = "MPE, y = (-4, 6)")
# grid()
```

## Boxplots of MPE for selected correlations
From the previous plots we may start discarding Beggs-Brill (MPE values go as far as -550%). Another one -observed in the scale (-4,100) could be Papp (their MPE go as far as +98%). 

In the plot of the center help us to eliminate the Shell correlation since it has MPE of above 30%.

We are left with Hall-Yarborough (HY), Dranchuk-ABouKassem (DAK), Dranchuk-Purvis-Robinson (DPR) and Neural-Network-10 (N10). Let's see them closer.

```{r}
library(zFactor)
qcorrs <- z.stats_quantile("MPE")
scorrs <- qcorrs[, c("HY", "DAK", "DPR", "N10")]

# par(mfrow = c(1,2))
boxplot(scorrs,  ylim= c(-2, 25), cex = 1.5, las=2, main = "MPE, y = (-2, 25)")
grid()

boxplot(scorrs,  ylim= c(-2, 2), cex = 1.5, las=2, main = "MPE, y = (-2, 2)")
# grid()
```

Still N10 is the correlation with less dispersion of the MPE. But any other three are still acceptable. DAK better than HY. 

If you are uncomfortable with the results of the neural network correlation, there is still a backup correlation to go to DAK. There is this little problem: neural network results are too good. But there is no physics and no experiment behind.

This leaves a lesson: 
